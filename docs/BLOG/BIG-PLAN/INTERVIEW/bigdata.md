# 海量数据处理的面试套路

经最近跳槽的小伙伴的反馈，发现面试官很喜欢问海量数据处理相关的问题，此类问题若是在面试中出现，其实是有套路可循的。

> 由于我们讨论的是面试的套路，故只谈方法/模式论，不深入细节中。

## 什么是海量数据处理？

海量数据处理，无非就是对海量数据的存储和计算。何谓海量，其实就是数据量太大，带来的问题是

- 无法在较短时间内快速计算完毕
- 无法一次性装入内存
> 针对短时间内无法快速计算，思路无非是采用巧妙的算法搭配合适的数据结构，如Bloom filter/Hash/bit-map/堆/倒排索引/trie树之类。

> 针对无法一次性装入内存，思路其实也很简单，无非是使用分而治之的思想，把数据集的规模降低到单机可处理的规模即可。

**以上思路，总结成以下套路**

1. 分而治之/hash映射 + hash统计 + 堆/快速/归并排序；
2. 双层桶划分；
3. Bloom filter/Bitmap；
4. Trie树/数据库/倒排索引；
5. 外排序；
6. 分布式处理之Hadoop/Mapreduce。
下面一个个说

### 套路1：分而治之/hash映射 + hash统计 + 堆/快速/归并排序

****1、海量日志数据，提取出某日访问百度次数最多的那个IP。****


既然是海量数据处理，那么可想而知，给我们的数据那就一定是海量的。针对这个数据的海量，我们如何着手呢?
如果把题目中的**海量**去掉，其实很简单，只需要对数据进行map统计即可。对海量数据来说，由于内存受限，很自然联想到的思路是：把大文件映射成小文件，缩小规模，逐个解决

#### 
解决方案：

7. 对IP进行sharding，由于IP可以转换为数字，故可直接对IP进行分区，举例对1000进行mod，分为100个区，如此划分，相同IP的数据会落入同一个小文件
8. 对每个小文件进行计算，hash<IP,Count>计算访问次数，求得当前文件最大值（这里其实也可以使用大顶堆来统计）
9. 取1000个小文件中最大值为访问次数最多的IP

这里其实有一些隐含的小问题

- 考虑下，如果出现数据倾斜，即单个IP的数据占比很高，那么即使对大文件进行sharding，由于相同IP都落到同一个切分后的文件，导致切分后的文件依然很大，还是加载不到内存中。这时候怎么解决呢？
- 针对数据倾斜，可以在对IP做sharding时加入随机值，使得数据分布尽可能的均匀，但会引入相同IP分布到不同分区的问题，导致原来的算法出现错误（单分区内的top1此时并不准了）
- 要解决这个问题，需要对数据做reduce处理，收集各个分区的hash<IP,Count>，做merge操作，将散落在各个分区的相同IP的count做合并
> 以上针对数据倾斜的处理，是已知了数据样本本身的数据分布的情况做出的特殊处理，若数据本身分布非常均匀，且数据区分度非常高，此时这种处理方式会导致最终merge节点OOM。如何规避这个问题？在处理数据集之前，进行数据集采样，当采样的结果证明数据可能出现数据倾斜时，才使用以上算法。
